{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, TypeVar, Dict, Any, Union"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE YOU WILL WRITE CODE TO TEST A NUMBER OF PREDICTORS\n",
    "# AND FINALLY CHOOSE AND TRAIN THE PREDICTOR THAT YOU WILL BE USING FOR PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initially prodding the data to find out about it\n",
    "\n",
    "Once again, the lack of any feature names is particularly unhelpful."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       F1      F2      F3     F4       F5     F6        F7     F8       F9  \\\n0  271.78  -21.92      UK -10.80  1899.57  17.98 -21749.82  91.94   855.61   \n1  202.54   43.82     USA -16.94  1941.57  -9.16 -27668.04  93.50   975.44   \n2  220.26   88.90  Europe -18.76  2298.12 -18.38 -11548.56  65.16  1114.28   \n3  141.00  140.72  Europe -19.86  -133.32 -57.00 -16200.96 -14.00   910.12   \n4  165.04    2.74  Europe -21.34  3077.07 -20.50 -25683.06  29.08   216.24   \n\n     F10  ...      F28     F29       F30   F31     F32     F33     F34  \\\n0  10.01  ...  2098.80  500.16  -6325.53  1.36  -33.14  177.34 -141.97   \n1   7.29  ...  1668.70  434.97  -6172.05  2.59  -58.87   87.48 -154.11   \n2  12.05  ...  2604.56  252.93 -10132.68  2.94  -40.89  271.00 -279.84   \n3   4.54  ...  2595.56  154.83  -7862.04  0.86 -117.03  201.66 -153.93   \n4  10.10  ...  1066.80  316.68  -6093.81  3.59  -63.84  211.82 -182.34   \n\n      F35  F36   Target  \n0  101.52    5   189.03  \n1  623.22    6   187.17  \n2  284.96    2  1016.24  \n3  532.19    4  -141.18  \n4  373.14    5    33.17  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F1</th>\n      <th>F2</th>\n      <th>F3</th>\n      <th>F4</th>\n      <th>F5</th>\n      <th>F6</th>\n      <th>F7</th>\n      <th>F8</th>\n      <th>F9</th>\n      <th>F10</th>\n      <th>...</th>\n      <th>F28</th>\n      <th>F29</th>\n      <th>F30</th>\n      <th>F31</th>\n      <th>F32</th>\n      <th>F33</th>\n      <th>F34</th>\n      <th>F35</th>\n      <th>F36</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>271.78</td>\n      <td>-21.92</td>\n      <td>UK</td>\n      <td>-10.80</td>\n      <td>1899.57</td>\n      <td>17.98</td>\n      <td>-21749.82</td>\n      <td>91.94</td>\n      <td>855.61</td>\n      <td>10.01</td>\n      <td>...</td>\n      <td>2098.80</td>\n      <td>500.16</td>\n      <td>-6325.53</td>\n      <td>1.36</td>\n      <td>-33.14</td>\n      <td>177.34</td>\n      <td>-141.97</td>\n      <td>101.52</td>\n      <td>5</td>\n      <td>189.03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>202.54</td>\n      <td>43.82</td>\n      <td>USA</td>\n      <td>-16.94</td>\n      <td>1941.57</td>\n      <td>-9.16</td>\n      <td>-27668.04</td>\n      <td>93.50</td>\n      <td>975.44</td>\n      <td>7.29</td>\n      <td>...</td>\n      <td>1668.70</td>\n      <td>434.97</td>\n      <td>-6172.05</td>\n      <td>2.59</td>\n      <td>-58.87</td>\n      <td>87.48</td>\n      <td>-154.11</td>\n      <td>623.22</td>\n      <td>6</td>\n      <td>187.17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220.26</td>\n      <td>88.90</td>\n      <td>Europe</td>\n      <td>-18.76</td>\n      <td>2298.12</td>\n      <td>-18.38</td>\n      <td>-11548.56</td>\n      <td>65.16</td>\n      <td>1114.28</td>\n      <td>12.05</td>\n      <td>...</td>\n      <td>2604.56</td>\n      <td>252.93</td>\n      <td>-10132.68</td>\n      <td>2.94</td>\n      <td>-40.89</td>\n      <td>271.00</td>\n      <td>-279.84</td>\n      <td>284.96</td>\n      <td>2</td>\n      <td>1016.24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>141.00</td>\n      <td>140.72</td>\n      <td>Europe</td>\n      <td>-19.86</td>\n      <td>-133.32</td>\n      <td>-57.00</td>\n      <td>-16200.96</td>\n      <td>-14.00</td>\n      <td>910.12</td>\n      <td>4.54</td>\n      <td>...</td>\n      <td>2595.56</td>\n      <td>154.83</td>\n      <td>-7862.04</td>\n      <td>0.86</td>\n      <td>-117.03</td>\n      <td>201.66</td>\n      <td>-153.93</td>\n      <td>532.19</td>\n      <td>4</td>\n      <td>-141.18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165.04</td>\n      <td>2.74</td>\n      <td>Europe</td>\n      <td>-21.34</td>\n      <td>3077.07</td>\n      <td>-20.50</td>\n      <td>-25683.06</td>\n      <td>29.08</td>\n      <td>216.24</td>\n      <td>10.10</td>\n      <td>...</td>\n      <td>1066.80</td>\n      <td>316.68</td>\n      <td>-6093.81</td>\n      <td>3.59</td>\n      <td>-63.84</td>\n      <td>211.82</td>\n      <td>-182.34</td>\n      <td>373.14</td>\n      <td>5</td>\n      <td>33.17</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_all_data: pd.DataFrame = pd.read_csv(\"CE802_P3_Data.csv\")\n",
    "_all_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "F1        float64\nF2        float64\nF3         object\nF4        float64\nF5        float64\nF6        float64\nF7        float64\nF8        float64\nF9        float64\nF10       float64\nF11        object\nF12       float64\nF13       float64\nF14       float64\nF15       float64\nF16       float64\nF17       float64\nF18       float64\nF19       float64\nF20       float64\nF21       float64\nF22       float64\nF23         int64\nF24       float64\nF25       float64\nF26       float64\nF27       float64\nF28       float64\nF29       float64\nF30       float64\nF31       float64\nF32       float64\nF33       float64\nF34       float64\nF35       float64\nF36         int64\nTarget    float64\ndtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_all_data.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "_all_data_inputs: pd.DataFrame = _all_data.loc[:,_all_data.columns!=\"Target\"]\n",
    "_all_data_targets:pd.DataFrame = _all_data.loc[:,\"Target\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like I'll need to convert F3 and F11 into a less awkward format.\n",
    "Of course, there's the simple approach of doing one-hot encoding, but..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UK' 'USA' 'Europe' 'Rest']\n",
      "['Very low' 'Medium' 'Low' 'Very high' 'High']\n"
     ]
    }
   ],
   "source": [
    "print(_all_data_inputs[\"F3\"].unique())\n",
    "\n",
    "print(_all_data_inputs[\"F11\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As F11 indicates values with some sequential nature to them\n",
    "(very low->low->medium->high->very high), I could get away with converting them\n",
    "to ints. Which is what I am going to do.\n",
    "\n",
    "However, for F3, I should probably just replace them with some one-hot encoding instead.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def df_f11_and_f3_to_sensible(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    f11dict = { \"F11\": {\n",
    "        \"Very low\": 0,\n",
    "        \"Low\": 1,\n",
    "        \"Medium\": 2,\n",
    "        \"High\": 3,\n",
    "        \"Very high\": 4\n",
    "    }}\n",
    "\n",
    "    df.replace(f11dict, inplace=True)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"F3\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "all_data = df_f11_and_f3_to_sensible(pd.read_csv(\"CE802_P3_Data.csv\"))\n",
    "\n",
    "_all_data_inputs = df_f11_and_f3_to_sensible(_all_data_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1           float64\n",
      "F2           float64\n",
      "F4           float64\n",
      "F5           float64\n",
      "F6           float64\n",
      "F7           float64\n",
      "F8           float64\n",
      "F9           float64\n",
      "F10          float64\n",
      "F11            int64\n",
      "F12          float64\n",
      "F13          float64\n",
      "F14          float64\n",
      "F15          float64\n",
      "F16          float64\n",
      "F17          float64\n",
      "F18          float64\n",
      "F19          float64\n",
      "F20          float64\n",
      "F21          float64\n",
      "F22          float64\n",
      "F23            int64\n",
      "F24          float64\n",
      "F25          float64\n",
      "F26          float64\n",
      "F27          float64\n",
      "F28          float64\n",
      "F29          float64\n",
      "F30          float64\n",
      "F31          float64\n",
      "F32          float64\n",
      "F33          float64\n",
      "F34          float64\n",
      "F35          float64\n",
      "F36            int64\n",
      "F3_Europe      uint8\n",
      "F3_Rest        uint8\n",
      "F3_UK          uint8\n",
      "F3_USA         uint8\n",
      "dtype: object\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "       F1      F2     F4       F5     F6        F7     F8       F9    F10  \\\n0  271.78  -21.92 -10.80  1899.57  17.98 -21749.82  91.94   855.61  10.01   \n1  202.54   43.82 -16.94  1941.57  -9.16 -27668.04  93.50   975.44   7.29   \n2  220.26   88.90 -18.76  2298.12 -18.38 -11548.56  65.16  1114.28  12.05   \n3  141.00  140.72 -19.86  -133.32 -57.00 -16200.96 -14.00   910.12   4.54   \n4  165.04    2.74 -21.34  3077.07 -20.50 -25683.06  29.08   216.24  10.10   \n\n   F11  ...   F31     F32     F33     F34     F35  F36  F3_Europe  F3_Rest  \\\n0    0  ...  1.36  -33.14  177.34 -141.97  101.52    5          0        0   \n1    2  ...  2.59  -58.87   87.48 -154.11  623.22    6          0        0   \n2    2  ...  2.94  -40.89  271.00 -279.84  284.96    2          1        0   \n3    1  ...  0.86 -117.03  201.66 -153.93  532.19    4          1        0   \n4    0  ...  3.59  -63.84  211.82 -182.34  373.14    5          1        0   \n\n   F3_UK  F3_USA  \n0      1       0  \n1      0       1  \n2      0       0  \n3      0       0  \n4      0       0  \n\n[5 rows x 39 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F1</th>\n      <th>F2</th>\n      <th>F4</th>\n      <th>F5</th>\n      <th>F6</th>\n      <th>F7</th>\n      <th>F8</th>\n      <th>F9</th>\n      <th>F10</th>\n      <th>F11</th>\n      <th>...</th>\n      <th>F31</th>\n      <th>F32</th>\n      <th>F33</th>\n      <th>F34</th>\n      <th>F35</th>\n      <th>F36</th>\n      <th>F3_Europe</th>\n      <th>F3_Rest</th>\n      <th>F3_UK</th>\n      <th>F3_USA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>271.78</td>\n      <td>-21.92</td>\n      <td>-10.80</td>\n      <td>1899.57</td>\n      <td>17.98</td>\n      <td>-21749.82</td>\n      <td>91.94</td>\n      <td>855.61</td>\n      <td>10.01</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.36</td>\n      <td>-33.14</td>\n      <td>177.34</td>\n      <td>-141.97</td>\n      <td>101.52</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>202.54</td>\n      <td>43.82</td>\n      <td>-16.94</td>\n      <td>1941.57</td>\n      <td>-9.16</td>\n      <td>-27668.04</td>\n      <td>93.50</td>\n      <td>975.44</td>\n      <td>7.29</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2.59</td>\n      <td>-58.87</td>\n      <td>87.48</td>\n      <td>-154.11</td>\n      <td>623.22</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220.26</td>\n      <td>88.90</td>\n      <td>-18.76</td>\n      <td>2298.12</td>\n      <td>-18.38</td>\n      <td>-11548.56</td>\n      <td>65.16</td>\n      <td>1114.28</td>\n      <td>12.05</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2.94</td>\n      <td>-40.89</td>\n      <td>271.00</td>\n      <td>-279.84</td>\n      <td>284.96</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>141.00</td>\n      <td>140.72</td>\n      <td>-19.86</td>\n      <td>-133.32</td>\n      <td>-57.00</td>\n      <td>-16200.96</td>\n      <td>-14.00</td>\n      <td>910.12</td>\n      <td>4.54</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.86</td>\n      <td>-117.03</td>\n      <td>201.66</td>\n      <td>-153.93</td>\n      <td>532.19</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165.04</td>\n      <td>2.74</td>\n      <td>-21.34</td>\n      <td>3077.07</td>\n      <td>-20.50</td>\n      <td>-25683.06</td>\n      <td>29.08</td>\n      <td>216.24</td>\n      <td>10.10</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3.59</td>\n      <td>-63.84</td>\n      <td>211.82</td>\n      <td>-182.34</td>\n      <td>373.14</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 39 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(_all_data_inputs.dtypes)\n",
    "print(\"\")\n",
    "_all_data_inputs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Much better.\n",
    "\n",
    "Anyway, here's a helper function that'll be useful later on,\n",
    "pretty much wrapping that dataframe reading/splitting/reformatting into a nice\n",
    "and simple function for ease of use later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def np_data_and_targets(df: pd.DataFrame = pd.read_csv(\"CE802_P3_Data.csv\"), already_formatted_f11_f3: bool = False, targetname: str = \"Target\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts dataframe into a couple of numpy ndarrays for the data without the labels,\n",
    "    and the labels by themselves.\n",
    "    Also handles the f11 and f3 formatting.\n",
    "    :param df: the Dataframe\n",
    "    :param already_formatted_f11_f3: set this to true if we've already replaced f11 and f3 with sensible formatting.\n",
    "    :param targetname: The name of the column holding the targets\n",
    "    :return: tuple of [ndarray of the values without the targets, just the class labels]\n",
    "    \"\"\"\n",
    "\n",
    "    inputs:  np.ndarray = df.loc[:,df.columns != targetname].to_numpy() if already_formatted_f11_f3 else df_f11_and_f3_to_sensible(df.loc[:,df.columns != targetname]).to_numpy()\n",
    "    outputs: np.ndarray =  df.loc[:,targetname].to_numpy()\n",
    "\n",
    "    return inputs, outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up a hold-out validation set for later\n",
    "\n",
    "Nested cross-validation moment.\n",
    "\n",
    "This validation set consists of 300 random items from the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "        F1      F2     F4       F5      F6        F7     F8       F9    F10  \\\n",
      "2   220.26   88.90 -18.76  2298.12  -18.38 -11548.56  65.16  1114.28  12.05   \n",
      "3   141.00  140.72 -19.86  -133.32  -57.00 -16200.96 -14.00   910.12   4.54   \n",
      "4   165.04    2.74 -21.34  3077.07  -20.50 -25683.06  29.08   216.24  10.10   \n",
      "5   247.64   26.68 -30.24  1868.22 -121.16 -10192.29  -9.68   428.42   9.07   \n",
      "16  145.36   85.52 -15.06  3658.14  -29.56 -24281.61  95.70   470.17   9.28   \n",
      "\n",
      "    F11  ...     F32     F33     F34     F35  F36   Target  F3_Europe  \\\n",
      "2     2  ...  -40.89  271.00 -279.84  284.96    2  1016.24          1   \n",
      "3     1  ... -117.03  201.66 -153.93  532.19    4  -141.18          1   \n",
      "4     0  ...  -63.84  211.82 -182.34  373.14    5    33.17          1   \n",
      "5     2  ... -132.50  223.08 -244.28  459.16    5   277.33          0   \n",
      "16    3  ...   -5.95  159.40 -145.85  349.06    6   387.40          0   \n",
      "\n",
      "    F3_Rest  F3_UK  F3_USA  \n",
      "2         0      0       0  \n",
      "3         0      0       0  \n",
      "4         0      0       0  \n",
      "5         1      0       0  \n",
      "16        0      0       1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "(300, 40)\n",
      "train\n",
      "       F1      F2     F4       F5     F6        F7      F8      F9    F10  \\\n",
      "0  271.78  -21.92 -10.80  1899.57  17.98 -21749.82   91.94  855.61  10.01   \n",
      "1  202.54   43.82 -16.94  1941.57  -9.16 -27668.04   93.50  975.44   7.29   \n",
      "6  166.76  -31.92 -27.18  1580.46 -58.42 -27216.24   31.98  468.47  11.86   \n",
      "7  111.98   98.36 -26.14  2148.12  74.62 -39245.52  122.58  470.72  10.79   \n",
      "8  200.42 -239.82 -19.46  2030.43 -12.30 -27179.04   65.72  203.12   1.25   \n",
      "\n",
      "   F11  ...    F32     F33     F34     F35  F36   Target  F3_Europe  F3_Rest  \\\n",
      "0    0  ... -33.14  177.34 -141.97  101.52    5   189.03          0        0   \n",
      "1    2  ... -58.87   87.48 -154.11  623.22    6   187.17          0        0   \n",
      "6    2  ...   1.74  209.94 -161.74  809.57    2   339.31          0        1   \n",
      "7    0  ... -26.74  171.40 -154.10  775.61    4    -4.52          0        1   \n",
      "8    4  ... -26.61  232.64 -196.92  825.80    3  1175.20          0        0   \n",
      "\n",
      "   F3_UK  F3_USA  \n",
      "0      1       0  \n",
      "1      0       1  \n",
      "6      0       0  \n",
      "7      0       0  \n",
      "8      1       0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "(1200, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "validation_cv: KFold = KFold(n_splits=5, shuffle=True)\n",
    "all_train_indices, validation_indices = [i for i in validation_cv.split(_all_data_inputs, _all_data_targets)][0]\n",
    "\n",
    "validation_df: pd.DataFrame = all_data.iloc[validation_indices]\n",
    "tests: Tuple[np.ndarray, np.ndarray] = np_data_and_targets(validation_df, True)\n",
    "test_data: np.ndarray = tests[0]\n",
    "test_labels: np.ndarray = tests[1]\n",
    "\n",
    "train_df: pd.DataFrame = all_data.iloc[all_train_indices]\n",
    "trains: Tuple[np.ndarray, np.ndarray] = np_data_and_targets(train_df, True)\n",
    "train_data: np.ndarray = trains[0]\n",
    "train_labels: np.ndarray = trains[1]\n",
    "\n",
    "print(\"validation\")\n",
    "print(validation_df.head())\n",
    "print(validation_df.shape)\n",
    "print(\"train\")\n",
    "print(train_df.head())\n",
    "print(train_df.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up some grid-search nested cross validation pipelines\n",
    "\n",
    "Once again, we're using HalvingGridSearchCV for the param_grid cross-validating search,\n",
    "because it's supposed to be faster than GridSearchCV, and speed like that is beneficial\n",
    "for my fragile ~~lack of~~ mental health.\n",
    "\n",
    "Unlike the classification tasks, there won't be any scaling happening here. This is because\n",
    "this particular task expects a real value to be output, instead of an abstract\n",
    "label or probability of a label being given. For classification tasks, scaling is\n",
    "generally a lot more appropriate, as abstract inputs can safely lead to abstract outputs.\n",
    "But, again, no abstract outputs means that abstract inputs could get problematic.\n",
    "And I don't want to have to deal with that headache.\n",
    "\n",
    "We're still using KNNImputer to impute missing features based on the features held by 'nearby' individuals,\n",
    "as that still seems like the most rational imputation method available given the circumstances,\n",
    "and is still a lot more elegant than simply discarding everything with a null value.\n",
    "\n",
    "Due to this task involving linear regression and the occasional problems of data leakage involved with\n",
    "linearly regressing on a high-dimensional dataset, I have attempted to compensate for this via the\n",
    "usage of a\n",
    "PolynomialFeatures preprocessor. Of course, I have no idea which parameter configuration\n",
    "for it is optimal, so I am trying to work this out via the grid search cross validation.\n",
    "Yes, the PolynomialFeatures does have the end result of 'addressing'\n",
    "the dimensionality issue by, uhh, adding more dimensonality.\n",
    "But this extra dimensionality mostly consists of multiplying the values\n",
    "in two of the 30-something dimensions together, in turn adding more dimensions in which\n",
    "each value has an impact, and, in theory, reducing the risk of certain values getting overlooked when\n",
    "they shouldn't.\n",
    "And if a value is still utterly unimportant, it can still be overlooked as usual!\n",
    "\n",
    "One other thing I should also mention is that I'm using the R^2 score\n",
    "to evaluate the performance of the parameter configurations for the grid search cv stuff.\n",
    "R^2 basically aggregates the differences between each prediction and the average expected value/the\n",
    "actual value, and represents them as a value that's <=1. If it's below 0, that means that the\n",
    "regressor isn't even getting the expected values correct, which is a major red flag. But anything\n",
    "above 0 is basically an improvement over random guessing.\n",
    "\n",
    "Finally, you might be wondering why the error score I'm using (for when there's an oopsie\n",
    "and the estimator completely fails to work) is a finite negative value. In short, I wanted to use\n",
    "negative infinity, but that caused warning messages to get plastered over the outputs in the notebook,\n",
    "complaining about the negative infinity value, so I had to compromise and put a stupidly big\n",
    "(in the negative sense) finite negative value in there for these situations instead. I'm working on\n",
    "the theory that if there is a situation that causes such a stupid value to be returned legitimately for\n",
    "the score, chances are that the problem of 'non-erroneous but utterly terrible result discarded in\n",
    "favour of an erroneous result' will be the least of everyone's concerns.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from math import inf\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "R = TypeVar('R', bound=RegressorMixin)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def h_grid_searcher(\n",
    "        regressor: R,\n",
    "        param_grid: Dict[str, List[Any]],\n",
    "        train_data: np.ndarray,\n",
    "        train_targets: np.ndarray,\n",
    "        k_fold: KFold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    ") -> HalvingGridSearchCV:\n",
    "\n",
    "\n",
    "    # last minute refactor to ensure that the Automatic Relevance Detection\n",
    "    # works in a reasonable legnth of time.\n",
    "\n",
    "    #param_grid[\"imputer__weights\"] = [\"distance\",\"uniform\"]\n",
    "    #param_grid[\"poly__degree\"] = [2,3]\n",
    "    #param_grid[\"poly__interaction_only\"] = [True, False]\n",
    "    #param_grid[\"poly__include_bias\"] = [True, False]\n",
    "\n",
    "    pl: Pipeline = Pipeline([\n",
    "        (\"imputer\",KNNImputer(add_indicator=True, weights=\"distance\")),\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"regressor\",regressor)\n",
    "    ])\n",
    "\n",
    "    h_grid_search: HalvingGridSearchCV = HalvingGridSearchCV(\n",
    "        estimator=pl,\n",
    "        param_grid=param_grid,\n",
    "        factor=3,\n",
    "        cv=k_fold,\n",
    "        scoring=make_scorer(r2_score),\n",
    "        refit=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        error_score=-1000000000000\n",
    "        # I wanted to make this error score negative infinity, however, doing so caused a lot of\n",
    "        # particularly unsightly warning messages to appear.\n",
    "\n",
    "        # So, to save everyone involved from having to look at at a buttload of them with a buttload of numbers in them,\n",
    "        # I'm just setting this to an incredibly low finite number which should be rather hard to reach.\n",
    "        # And if this score (or an even lower score) somehow is reached legitimately, chances are that\n",
    "        # the legitimate score being lower than the error score will be the least of one's concerns.\n",
    "    )\n",
    "\n",
    "    h_grid_search.fit(train_data, train_targets)\n",
    "\n",
    "    return h_grid_search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def nested_h_grid_searcher(\n",
    "        regressor: R,\n",
    "        param_grid: Dict[str, List[Any]],\n",
    "        train_data: np.ndarray,\n",
    "        train_targets: np.ndarray,\n",
    "        t_df: pd.DataFrame,\n",
    "        kfold_splits: int = 6\n",
    ") -> Dict[HalvingGridSearchCV, float]:\n",
    "\n",
    "    h_grid_search_dicts: Dict[HalvingGridSearchCV, float] = {}\n",
    "\n",
    "    kf: KFold = KFold(n_splits=kfold_splits, shuffle=True)\n",
    "\n",
    "    for train_indices, test_indices in kf.split(train_data, train_targets):\n",
    "        tr: Tuple[np.ndarray, np.ndarray] = np_data_and_targets(t_df.iloc[train_indices],True)\n",
    "        te: Tuple[np.ndarray, np.ndarray] = np_data_and_targets(t_df.iloc[test_indices],True)\n",
    "\n",
    "        current_search: HalvingGridSearchCV = h_grid_searcher(\n",
    "            regressor,\n",
    "            param_grid,\n",
    "            tr[0],\n",
    "            tr[1],\n",
    "            KFold(n_splits=kfold_splits-1, shuffle=False)\n",
    "        )\n",
    "\n",
    "        current_score: float = current_search.score(te[0],te[1])\n",
    "\n",
    "        h_grid_search_dicts[current_search] = current_score\n",
    "\n",
    "    return h_grid_search_dicts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression\n",
    "\n",
    "There isn't much that I can do in terms of hyper-parameter search for simple linear regression,\n",
    "but, here goes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 111\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 111\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 333\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_h_grid_searcher(\n",
    "    LinearRegression(),\n",
    "    {\n",
    "        \"poly__degree\": [2,3],\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False],\n",
    "        \"imputer__weights\": [\"distance\",\"uniform\"]\n",
    "    },\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    train_df\n",
    ")\n",
    "\n",
    "linreg_searched: HalvingGridSearchCV = max(\n",
    "    linear_regression_searched_dict.keys(), key=lambda k: linear_regression_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_linreg: Tuple[HalvingGridSearchCV, float] = (\n",
    "    linreg_searched, linear_regression_searched_dict[linreg_searched]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(HalvingGridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
      "                    error_score=-1000000000000,\n",
      "                    estimator=Pipeline(steps=[('imputer',\n",
      "                                               KNNImputer(add_indicator=True)),\n",
      "                                              ('poly', PolynomialFeatures()),\n",
      "                                              ('regressor',\n",
      "                                               LinearRegression())]),\n",
      "                    n_jobs=-1,\n",
      "                    param_grid={'imputer__weights': ['distance', 'uniform'],\n",
      "                                'poly__degree': [2, 3],\n",
      "                                'poly__include_bias': [True, False],\n",
      "                                'poly__interaction_only': [True, False]},\n",
      "                    refit=<function _refit_callable at 0x00000292BE6995E0>,\n",
      "                    scoring=make_scorer(r2_score), verbose=1), -7.0379606171048525)\n",
      "-7.0379606171048525\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('imputer', KNNImputer(add_indicator=True, weights='distance')),\n                ('poly',\n                 PolynomialFeatures(degree=3, include_bias=False,\n                                    interaction_only=True)),\n                ('regressor', LinearRegression())])",
      "text/html": "<style>#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b {color: black;background-color: white;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b pre{padding: 0;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-toggleable {background-color: white;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-estimator:hover {background-color: #d4ebff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-item {z-index: 1;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-parallel-item:only-child::after {width: 0;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-e98e3585-ad70-4c4f-a864-70edc6909b7b div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-e98e3585-ad70-4c4f-a864-70edc6909b7b\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ab31365a-149d-4856-87a0-50f94aa744b9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ab31365a-149d-4856-87a0-50f94aa744b9\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('imputer', KNNImputer(add_indicator=True, weights='distance')),\n                ('poly',\n                 PolynomialFeatures(degree=3, include_bias=False,\n                                    interaction_only=True)),\n                ('regressor', LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6f4723a1-fd10-4320-a2f4-89d25e8adf67\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6f4723a1-fd10-4320-a2f4-89d25e8adf67\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer(add_indicator=True, weights='distance')</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d45e2f2c-c70e-43ca-8e81-7e20a5daeaeb\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d45e2f2c-c70e-43ca-8e81-7e20a5daeaeb\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(degree=3, include_bias=False, interaction_only=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b5801ad2-886a-4a25-bb9a-09977525175e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b5801ad2-886a-4a25-bb9a-09977525175e\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_linreg)\n",
    "\n",
    "print(best_linreg[1])\n",
    "\n",
    "linreg_searched.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RANdom SAmple Consensus (RANSAC) regression\n",
    "\n",
    "This is an adaption of the linear regression method, of the 'robust regressor' variety.\n",
    "This means it basically copes with outliers a lot better than a simple linear regressor would,\n",
    "although, in spaces of higher dimensionality (such as this problem), it apparently can struggle.\n",
    "It deals with outliers by, in short, completely ignoring them.\n",
    "\n",
    "It's an iterative approach, randomly sampling several items from the population and\n",
    "seeing if they're 'valid' data or not, trying to fit a model to that random subset,\n",
    "classifiying them as inliers/outliers appropriately, saving the current model as the 'best' model\n",
    "if it has more inliers than the previous model (if same inliers, saving if it has a better score),\n",
    "and repeating for a certain number of trials, or until reaching a certain threshold of inliers\n",
    "or score threshold.\n",
    "\n",
    "For this situation, I have defined a score threshold of 1 (being the largest possible R2 score)\n",
    "for a premature stop. Of course, I could have defined a score threshold of 0.95 or 0.99\n",
    "etc as being good enough, but I figured that as there was very little chance of reaching those\n",
    "R2 values anyway, I may as well go all-in and aim for an R2 value of 1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 10\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 768\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 256\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 86\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 86 candidates, totalling 430 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 29\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 10\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "(HalvingGridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
      "                    error_score=-1000000000000,\n",
      "                    estimator=Pipeline(steps=[('imputer',\n",
      "                                               KNNImputer(add_indicator=True)),\n",
      "                                              ('poly', PolynomialFeatures()),\n",
      "                                              ('regressor',\n",
      "                                               RANSACRegressor(base_estimator=LinearRegression(),\n",
      "                                                               stop_score=1))]),\n",
      "                    n_jobs=-1,\n",
      "                    param_grid={'imputer__weights': ['distance', 'uniform'],\n",
      "                                'poly__degree': [2, 3],\n",
      "                                'poly__include_bias': [True, False],\n",
      "                                'poly__interaction_only': [True, False],\n",
      "                                'regressor__max_trials': [80, 100, 120],\n",
      "                                'regressor__min_samples': [None, 2, 4, 8],\n",
      "                                'regressor__residual_threshold': [None, 100,\n",
      "                                                                  1000,\n",
      "                                                                  10000]},\n",
      "                    refit=<function _refit_callable at 0x00000292BE6995E0>,\n",
      "                    scoring=make_scorer(r2_score), verbose=1), -0.08791271925419597)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "rs_samples: List[Union[int, None]] = [None, 2, 4, 8]\n",
    "rs_max_trials: List[int] = [80,100,120]\n",
    "rs_residual: List[Union[float, None]] = [None,100, 1000, 10000]\n",
    "\n",
    "rs_param_dict: Dict[str, List[Any]] = {\n",
    "    \"regressor__min_samples\": rs_samples,\n",
    "    \"regressor__max_trials\": rs_max_trials,\n",
    "    \"regressor__residual_threshold\": rs_residual,\n",
    "    \"poly__degree\": [2,3],\n",
    "    \"poly__interaction_only\": [True, False],\n",
    "    \"poly__include_bias\": [True, False],\n",
    "    \"imputer__weights\": [\"distance\",\"uniform\"]\n",
    "}\n",
    "\n",
    "ransac_regression_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_h_grid_searcher(\n",
    "    RANSACRegressor(base_estimator=LinearRegression(), stop_score=1),\n",
    "    rs_param_dict,\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    train_df\n",
    ")\n",
    "\n",
    "ransac_searched: HalvingGridSearchCV = max(\n",
    "    ransac_regression_searched_dict.keys(), key=lambda k: ransac_regression_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_ransac: Tuple[HalvingGridSearchCV, float] = (\n",
    "    ransac_searched, ransac_regression_searched_dict[ransac_searched]\n",
    ")\n",
    "\n",
    "print(best_ransac)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08791271925419597\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('imputer', KNNImputer(add_indicator=True)),\n                ('poly', PolynomialFeatures(interaction_only=True)),\n                ('regressor',\n                 RANSACRegressor(base_estimator=LinearRegression(),\n                                 max_trials=80, min_samples=4,\n                                 residual_threshold=100, stop_score=1))])",
      "text/html": "<style>#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf {color: black;background-color: white;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf pre{padding: 0;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-toggleable {background-color: white;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-estimator:hover {background-color: #d4ebff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-item {z-index: 1;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-parallel-item:only-child::after {width: 0;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-cde1dc07-38cc-4a23-8ed9-d12fdce50cbf\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c4a77aee-f165-481b-a075-792a3a728896\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c4a77aee-f165-481b-a075-792a3a728896\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('imputer', KNNImputer(add_indicator=True)),\n                ('poly', PolynomialFeatures(interaction_only=True)),\n                ('regressor',\n                 RANSACRegressor(base_estimator=LinearRegression(),\n                                 max_trials=80, min_samples=4,\n                                 residual_threshold=100, stop_score=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ea1acde3-a444-4b6c-9dfd-d9bf0ac10c06\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ea1acde3-a444-4b6c-9dfd-d9bf0ac10c06\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer(add_indicator=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a1135f97-80c8-47cd-ae4f-d03598bdbdda\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a1135f97-80c8-47cd-ae4f-d03598bdbdda\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(interaction_only=True)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"383ba7d5-248e-4b93-b17f-bf5ca7ec9f5d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"383ba7d5-248e-4b93-b17f-bf5ca7ec9f5d\">regressor: RANSACRegressor</label><div class=\"sk-toggleable__content\"><pre>RANSACRegressor(base_estimator=LinearRegression(), max_trials=80, min_samples=4,\n                residual_threshold=100, stop_score=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bf7d7018-1f9c-4e20-ac3a-7a124b8590aa\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bf7d7018-1f9c-4e20-ac3a-7a124b8590aa\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_ransac[1])\n",
    "\n",
    "ransac_searched.best_estimator_\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now, time for something completely different.\n",
    "\n",
    "# Relevance Vector Machine - Bayesian Ridge Regression with Automatic Relevance Detection (ARD)\n",
    "\n",
    "This attempts to use Bayesian logic to produce a probabilistic model of the regression problem,\n",
    "inherently also regularizing the data being fed into it. The regularization is based\n",
    "on three 'alpha' and 'lambda' parameters, with one alpha being responsible\n",
    "for declaring the precision of the noise, one lambda setting the precision of the weights,\n",
    "and the other two of each being responsible for declaring how the initial alpha/lambda get\n",
    "adjusted over time. In the case of the ARD regressor, only the supplementary alpha and lambda\n",
    "need to be declared, as the ARD regressor will attempt to estimate the main alpha/lambda itself.\n",
    "\n",
    "It attempts to perform this est\n",
    "\n",
    "One thing that must be taken into account with this variety of regression is that it takes a\n",
    "rather long time to train. Therefore, in the interests of completing this investigation within a\n",
    "reasonable length of time, I had to perform some last-minute refactors of the param search code\n",
    "within the cross-validator methods to remove the automatic investigation of different\n",
    "KNNImputer and PolynomialFeatures parameters, moving the code putting that stuff into the\n",
    "grid_param_dicts into the definition of their grid_param_dicts themselves (the end result\n",
    "being that those code cells still do the same overall thing, and this investigation doesn't\n",
    "have that bit of investigation tacked onto it).\n",
    "\n",
    "Furthermore, I initially planned to look into the effects of different values for\n",
    "`tol` (used to determine when the algorithm considers convergence to have happened)\n",
    "and `threshold_lambda` (used for pruning), but, due to time constraints (both in terms of\n",
    "deadline time constraints, and time I can physically spend waiting for this to run on a\n",
    "computer, with decent hardware, which is acting particularly sluggish (to the point of being unable\n",
    "to do any other work in the meantime on it) due to having to process this), those investigations\n",
    "had to be unceremoniously scrapped, instead focusing the investigation on changing the powers\n",
    "of the alpha and lambda (1 and 2) hyper-parameters.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 17 iterations\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 15 iterations\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 39 iterations\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 34 iterations\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 16 iterations\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 12\n",
      "max_resources_: 1000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 81\n",
      "n_resources: 12\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 36\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 9\n",
      "n_resources: 108\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 324\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 1\n",
      "n_resources: 972\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Converged after 25 iterations\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ARDRegression, BayesianRidge\n",
    "\n",
    "ard_tol: List[float] = [1e-2, 1e-3, 1e-4]\n",
    "ard_alpha_lambda: List[float] = [1e-5, 1e-6, 1e-7]\n",
    "ard_thresh_lambda: List[float] = [7500, 10000, 12500]\n",
    "\n",
    "ard_param_dict: Dict[str, List[Any]] = {\n",
    "    #\"regressor__tol\": ard_tol,\n",
    "    \"regressor__alpha_1\" : ard_alpha_lambda,\n",
    "    \"regressor__alpha_2\" : ard_alpha_lambda,\n",
    "    \"regressor__lambda_1\" : ard_alpha_lambda,\n",
    "    \"regressor__lambda_2\" : ard_alpha_lambda,\n",
    "    #\"regressor__threshold_lambda\": ard_thresh_lambda\n",
    "}\n",
    "\n",
    "# had to comment out the investigations into the tolerance values and pruning threshold lambdas\n",
    "# in the interests of being able to get this grid_param_search done within a half-reasonable amount of time.\n",
    "\n",
    "\n",
    "ard_grid_search_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_h_grid_searcher(\n",
    "    ARDRegression(n_iter=300, verbose=True),\n",
    "    ard_param_dict,\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    train_df\n",
    ")\n",
    "\n",
    "ard_searched: HalvingGridSearchCV = max(\n",
    "    ard_grid_search_dict.keys(), key=lambda k: ard_grid_search_dict[k]\n",
    ")\n",
    "\n",
    "best_ard: Tuple[HalvingGridSearchCV, float] = (\n",
    "    ard_searched, ard_grid_search_dict[ard_searched]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(HalvingGridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
      "                    error_score=-1000000000000,\n",
      "                    estimator=Pipeline(steps=[('imputer',\n",
      "                                               KNNImputer(add_indicator=True,\n",
      "                                                          weights='distance')),\n",
      "                                              ('poly', PolynomialFeatures()),\n",
      "                                              ('regressor',\n",
      "                                               ARDRegression(verbose=True))]),\n",
      "                    n_jobs=-1,\n",
      "                    param_grid={'regressor__alpha_1': [1e-05, 1e-06, 1e-07],\n",
      "                                'regressor__alpha_2': [1e-05, 1e-06, 1e-07],\n",
      "                                'regressor__lambda_1': [1e-05, 1e-06, 1e-07],\n",
      "                                'regressor__lambda_2': [1e-05, 1e-06, 1e-07]},\n",
      "                    refit=<function _refit_callable at 0x00000292BE6995E0>,\n",
      "                    scoring=make_scorer(r2_score), verbose=1), 0.6700347296516445)\n",
      "0.6700347296516445\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('imputer', KNNImputer(add_indicator=True, weights='distance')),\n                ('poly', PolynomialFeatures()),\n                ('regressor',\n                 ARDRegression(alpha_2=1e-07, lambda_2=1e-07, verbose=True))])",
      "text/html": "<style>#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 {color: black;background-color: white;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 pre{padding: 0;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-toggleable {background-color: white;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-item {z-index: 1;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-parallel-item:only-child::after {width: 0;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-36b3fc29-1245-4a87-947a-50962eb4fb12 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-36b3fc29-1245-4a87-947a-50962eb4fb12\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"70ccf94e-4715-4131-991a-c8b1870bda81\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"70ccf94e-4715-4131-991a-c8b1870bda81\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('imputer', KNNImputer(add_indicator=True, weights='distance')),\n                ('poly', PolynomialFeatures()),\n                ('regressor',\n                 ARDRegression(alpha_2=1e-07, lambda_2=1e-07, verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d3232e47-27de-47df-bdc0-a31e96d27119\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d3232e47-27de-47df-bdc0-a31e96d27119\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer(add_indicator=True, weights='distance')</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"28e0da86-9cca-4ad2-8d30-dcc55bd2b7ae\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"28e0da86-9cca-4ad2-8d30-dcc55bd2b7ae\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2bd8523d-1ca1-4412-9005-aedf4acc2423\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2bd8523d-1ca1-4412-9005-aedf4acc2423\">ARDRegression</label><div class=\"sk-toggleable__content\"><pre>ARDRegression(alpha_2=1e-07, lambda_2=1e-07, verbose=True)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_ard)\n",
    "\n",
    "print(best_ard[1])\n",
    "\n",
    "ard_searched.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attempting to run each estimator on my held-out test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "tests: Tuple[np.ndarray, np.ndarray] = np_data_and_targets(validation_df, True)\n",
    "test_data: np.ndarray = tests[0]\n",
    "test_labels: np.ndarray = tests[1]\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.497730104411637\n"
     ]
    }
   ],
   "source": [
    "linreg_score: float = linreg_searched.score(test_data, test_labels)\n",
    "print(linreg_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10247452886073871\n"
     ]
    }
   ],
   "source": [
    "ransac_score: float = ransac_searched.score(test_data, test_labels)\n",
    "print(ransac_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6946717708240002\n"
     ]
    }
   ],
   "source": [
    "ard_score: float = ard_searched.score(test_data, test_labels)\n",
    "print(ard_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best regressor is...\n",
      "Relevance Vector Machine - Bayesian Ridge Regression with Automatic Relevance Detection\n",
      "\n",
      "Overall R score of 0.6946717708240002\n"
     ]
    }
   ],
   "source": [
    "all_regressors: List[Tuple[HalvingGridSearchCV, float, str]] = [\n",
    "    (linreg_searched, linreg_score, \"Linear regression\"),\n",
    "    (ransac_searched, ransac_score, \"RANdom SAmple Consensus regression\"),\n",
    "    (ard_searched, ard_score, \"Relevance Vector Machine - Bayesian Ridge Regression with Automatic Relevance Detection\")\n",
    "]\n",
    "\n",
    "best_regressor: Tuple[HalvingGridSearchCV, float, str] = max(\n",
    "    all_regressors, key=lambda psn: psn[1]\n",
    ")\n",
    "\n",
    "print(\"The best regressor is...\")\n",
    "print(best_regressor[2])\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Overall R score of {best_regressor[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# HERE YOU WILL USE THIS TEMPLATE TO SAVE THE PREDICTIONS ON THE TEST SET\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('CE802_P3_Test.csv')\n",
    "\n",
    "final_data, final_targets = np_data_and_targets(pd.read_csv(\"CE802_P3_Test.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "predicted = best_regressor[0].predict(final_data)\n",
    "# CHANGE HERE -- use your previously trained predictor and apply it to test_data\n",
    "# (test_data can be modified if needed but make sure you don't change the order of the rows)...\n",
    "\n",
    "# Replace the last (empty) column with your prediction\n",
    "test_df.iloc[:,-1] = predicted\n",
    "\n",
    "\n",
    "\n",
    "# Save to the destination file\n",
    "test_df.to_csv('CE802_P3_Test_Predictions.csv', index=False, float_format='%.8g')\n",
    "\n",
    "# IMPORTANT!! Make sure only the last column has changed\n",
    "assert pd.read_csv('CE802_P3_Test.csv').iloc[:,:-1].equals(pd.read_csv('CE802_P3_Test_Predictions.csv').iloc[:,:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}